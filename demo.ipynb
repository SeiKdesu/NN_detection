{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from neural_interaction_detection import get_interactions\n",
    "from multilayer_perceptron import MLP, train, get_weights\n",
    "from utils import (\n",
    "    preprocess_data,\n",
    "    get_pairwise_auc,\n",
    "    get_anyorder_R_precision,\n",
    "    set_seed,\n",
    "    print_rankings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_main_effect_nets = True  # toggle this to use \"main effect\" nets\n",
    "num_samples = 1000\n",
    "num_features = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic data with ground truth interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "BLOCK_DIM = 25\n",
    "NUM_BLOCKS = 4\n",
    "TOTAL_DIM = BLOCK_DIM * NUM_BLOCKS\n",
    "\n",
    "# ---- py-benchmark integration ----\n",
    "# NOTE: function names must exist in your py-benchmark installation.\n",
    "# Edit POOL_NAMES to match the available function names in your environment.\n",
    "POOL_NAMES = [\n",
    "    \"sphere\",\n",
    "    \"rastrigin\",\n",
    "    \"ackley\",\n",
    "    \"rosenbrock\",\n",
    "    \"griewank\",\n",
    "    \"schwefel\",\n",
    "    \"levy\",\n",
    "    \"zakharov\",\n",
    "    \"michalewicz\",\n",
    "    \"dixon_price\",\n",
    "    \"sum_squares\",\n",
    "    \"bent_cigar\",\n",
    "    \"discus\",\n",
    "    \"weierstrass\",\n",
    "    \"ellipsoid\",\n",
    "    \"alpine1\",\n",
    "    \"alpine2\",\n",
    "    \"katsuura\",\n",
    "    \"salomon\",\n",
    "    \"whitley\",\n",
    "    \"bohachevsky\",\n",
    "    \"perm\",\n",
    "    \"trid\",\n",
    "    \"powell\",\n",
    "    \"styblinski_tang\",\n",
    "]\n",
    "\n",
    "\n",
    "def _import_pybenchmark():\n",
    "    # Try common module names used by py-benchmark distributions.\n",
    "    for mod in (\"pybenchmarks\", \"py_benchmark\", \"pybenchmark\", \"pybench\"):\n",
    "        try:\n",
    "            return __import__(mod)\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise ModuleNotFoundError(\n",
    "        \"py-benchmark is required. Install it and ensure its module name is one of \"\n",
    "        \"[pybenchmarks, py_benchmark, pybenchmark, pybench], or edit _import_pybenchmark().\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _random_rotation_matrix(dim, rng):\n",
    "    H = rng.normal(size=(dim, dim))\n",
    "    Q, _ = np.linalg.qr(H)\n",
    "    if np.linalg.det(Q) < 0:\n",
    "        Q[:, 0] *= -1\n",
    "    return Q\n",
    "\n",
    "\n",
    "def _resolve_shift(shift, rng, dim, scale):\n",
    "    if shift is None or shift is False:\n",
    "        return None\n",
    "    if isinstance(shift, str):\n",
    "        if shift == \"random\":\n",
    "            return rng.uniform(-scale, scale, size=dim)\n",
    "        raise ValueError(f\"Unknown shift spec: {shift}\")\n",
    "    shift = np.asarray(shift, dtype=float)\n",
    "    if shift.shape != (dim,):\n",
    "        raise ValueError(f\"shift must be shape ({dim},), got {shift.shape}\")\n",
    "    return shift\n",
    "\n",
    "\n",
    "def _resolve_rotation(rotate, rng, dim):\n",
    "    if rotate is None or rotate is False:\n",
    "        return None\n",
    "    if rotate is True or rotate == \"random\":\n",
    "        return _random_rotation_matrix(dim, rng)\n",
    "    rotate = np.asarray(rotate, dtype=float)\n",
    "    if rotate.shape != (dim, dim):\n",
    "        raise ValueError(f\"rotation must be shape ({dim}, {dim}), got {rotate.shape}\")\n",
    "    return rotate\n",
    "\n",
    "\n",
    "def _apply_transform(X, shift=None, rotation=None):\n",
    "    X_t = X\n",
    "    if shift is not None:\n",
    "        X_t = X_t - shift\n",
    "    if rotation is not None:\n",
    "        X_t = X_t @ rotation\n",
    "    return X_t\n",
    "\n",
    "\n",
    "def _wrap_callable(fn, dim):\n",
    "    # Normalize to a callable that accepts X with shape (N, dim) and returns (N,)\n",
    "    # py-benchmark functions often accept a single vector; we adapt as needed.\n",
    "    def _eval(X):\n",
    "        X = np.asarray(X)\n",
    "        if X.ndim != 2 or X.shape[1] != dim:\n",
    "            raise ValueError(f\"Expected X shape (N, {dim}), got {X.shape}\")\n",
    "        try:\n",
    "            y = fn(X)\n",
    "        except Exception:\n",
    "            y = np.array([fn(x) for x in X])\n",
    "        y = np.asarray(y)\n",
    "        if y.ndim == 2 and y.shape[1] == 1:\n",
    "            y = y[:, 0]\n",
    "        if y.ndim == 0:\n",
    "            y = np.full(X.shape[0], y)\n",
    "        if y.ndim != 1:\n",
    "            raise ValueError(\"Benchmark function must return (N,) or scalar per row\")\n",
    "        return y\n",
    "\n",
    "    return _eval\n",
    "\n",
    "\n",
    "def _get_pybenchmark_callable(name, dim, module=None):\n",
    "    module = module or _import_pybenchmark()\n",
    "    last_err = None\n",
    "\n",
    "    # 1) Factory-style functions\n",
    "    for attr in (\"get_function\", \"get_benchmark\", \"get_benchmark_function\", \"get\"):\n",
    "        if hasattr(module, attr):\n",
    "            try:\n",
    "                candidate = getattr(module, attr)(name, dim)\n",
    "                return _wrap_callable(candidate, dim)\n",
    "            except Exception as exc:\n",
    "                last_err = exc\n",
    "\n",
    "    # 2) Submodules with named callables/classes\n",
    "    for sub in (\"functions\", \"benchmarks\", \"benchmark\", \"funcs\"):\n",
    "        if hasattr(module, sub):\n",
    "            submod = getattr(module, sub)\n",
    "            if hasattr(submod, name):\n",
    "                obj = getattr(submod, name)\n",
    "                try:\n",
    "                    obj = obj(dim)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                return _wrap_callable(obj, dim)\n",
    "\n",
    "    # 3) Direct attribute\n",
    "    if hasattr(module, name):\n",
    "        obj = getattr(module, name)\n",
    "        try:\n",
    "            obj = obj(dim)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return _wrap_callable(obj, dim)\n",
    "\n",
    "    if last_err is not None:\n",
    "        raise ValueError(f\"Function '{name}' not found or unsupported: {last_err}\")\n",
    "    raise ValueError(f\"Function '{name}' not found in py-benchmark module: {module}\")\n",
    "\n",
    "\n",
    "def make_pool_specs(names, rotate=True, shift=\"random\", seed_base=0, weight=1.0, interaction=True):\n",
    "    specs = []\n",
    "    for i, name in enumerate(names):\n",
    "        specs.append(\n",
    "            {\n",
    "                \"name\": name,\n",
    "                \"rotate\": rotate,\n",
    "                \"shift\": shift,\n",
    "                \"seed\": seed_base + i,\n",
    "                \"weight\": weight,\n",
    "                \"interaction\": interaction,\n",
    "            }\n",
    "        )\n",
    "    return specs\n",
    "\n",
    "\n",
    "def build_benchmark_pool(specs, dim, seed=123, shift_scale=1.0, module=None):\n",
    "    module = module or _import_pybenchmark()\n",
    "    rng_master = np.random.default_rng(seed)\n",
    "\n",
    "    pool = []\n",
    "    for spec in specs:\n",
    "        spec = dict(spec)\n",
    "        rng = np.random.default_rng(spec.get(\"seed\", rng_master.integers(0, 2**32 - 1)))\n",
    "        shift = _resolve_shift(spec.get(\"shift\", None), rng, dim, shift_scale)\n",
    "        rotation = _resolve_rotation(spec.get(\"rotate\", False), rng, dim)\n",
    "\n",
    "        fn = _get_pybenchmark_callable(spec[\"name\"], dim, module=module)\n",
    "        pool.append(\n",
    "            {\n",
    "                \"name\": spec[\"name\"],\n",
    "                \"fn\": fn,\n",
    "                \"shift\": shift,\n",
    "                \"rotation\": rotation,\n",
    "                \"weight\": spec.get(\"weight\", 1.0),\n",
    "                \"interaction\": spec.get(\"interaction\", True),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pool\n",
    "\n",
    "\n",
    "def indices_from_names(pool, names):\n",
    "    name_to_index = {spec[\"name\"]: i for i, spec in enumerate(pool)}\n",
    "    missing = [n for n in names if n not in name_to_index]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Names not in pool: {missing}\")\n",
    "    return [name_to_index[n] for n in names]\n",
    "\n",
    "\n",
    "def make_composite_objective(pool, selected_indices, block_dim=BLOCK_DIM, num_blocks=NUM_BLOCKS):\n",
    "    selected_indices = list(selected_indices)\n",
    "    if len(selected_indices) != num_blocks:\n",
    "        raise ValueError(f\"selected_indices must be length {num_blocks}\")\n",
    "\n",
    "    def _objective(X):\n",
    "        X = np.asarray(X)\n",
    "        if X.ndim != 2 or X.shape[1] != block_dim * num_blocks:\n",
    "            raise ValueError(f\"X must be (N, {block_dim * num_blocks}). Got {X.shape}\")\n",
    "\n",
    "        Y = np.zeros(X.shape[0], dtype=float)\n",
    "        ground_truth = []\n",
    "        for block_i, pool_i in enumerate(selected_indices):\n",
    "            spec = pool[pool_i]\n",
    "            X_block = X[:, block_i * block_dim : (block_i + 1) * block_dim]\n",
    "            X_block = _apply_transform(X_block, spec[\"shift\"], spec[\"rotation\"])\n",
    "            Y = Y + spec[\"weight\"] * spec[\"fn\"](X_block)\n",
    "\n",
    "            if spec.get(\"interaction\", True):\n",
    "                start = block_i * block_dim + 1\n",
    "                end = (block_i + 1) * block_dim + 1\n",
    "                ground_truth.append(set(range(start, end)))\n",
    "\n",
    "        return Y, ground_truth\n",
    "\n",
    "    return _objective\n",
    "\n",
    "\n",
    "# ---- User-configurable pool and selection ----\n",
    "POOL_SPECS = make_pool_specs(POOL_NAMES, rotate=True, shift=\"random\", seed_base=0)\n",
    "BENCHMARK_POOL = build_benchmark_pool(POOL_SPECS, dim=BLOCK_DIM, seed=123, shift_scale=1.0)\n",
    "\n",
    "# Select 4 functions (each 25D) to compose a 100D objective.\n",
    "# Option A: select by indices\n",
    "SELECTED_POOL_INDICES = [0, 1, 2, 3]\n",
    "\n",
    "# Option B: select by names (uncomment if you prefer names and have unique names)\n",
    "# SELECTED_POOL_NAMES = [\"sphere\", \"rastrigin\", \"ackley\", \"rosenbrock\"]\n",
    "# SELECTED_POOL_INDICES = indices_from_names(BENCHMARK_POOL, SELECTED_POOL_NAMES)\n",
    "\n",
    "synth_func = make_composite_objective(BENCHMARK_POOL, SELECTED_POOL_INDICES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "X = np.random.uniform(low=-1, high=1, size=(num_samples, num_features))\n",
    "Y, ground_truth = synth_func(X)\n",
    "data_loaders = preprocess_data(\n",
    "    X, Y, valid_size=100, test_size=100, std_scale=True, get_torch_loaders=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = MLP(\n",
    "    num_features, [140, 100, 60, 20], use_main_effect_nets=use_main_effect_nets\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to train\n",
      "early stopping enabled\n",
      "[epoch 1, total 100] train loss: 3.4727, val loss: 1.0503\n",
      "[epoch 3, total 100] train loss: 1.1290, val loss: 0.9284\n",
      "[epoch 5, total 100] train loss: 0.9798, val loss: 0.8819\n",
      "[epoch 7, total 100] train loss: 0.8218, val loss: 0.7768\n",
      "[epoch 9, total 100] train loss: 0.5839, val loss: 0.4960\n",
      "[epoch 11, total 100] train loss: 0.1865, val loss: 0.1349\n",
      "[epoch 13, total 100] train loss: 0.0822, val loss: 0.0891\n",
      "[epoch 15, total 100] train loss: 0.0501, val loss: 0.0641\n",
      "[epoch 17, total 100] train loss: 0.0296, val loss: 0.0449\n",
      "[epoch 19, total 100] train loss: 0.0201, val loss: 0.0311\n",
      "[epoch 21, total 100] train loss: 0.0148, val loss: 0.0258\n",
      "[epoch 23, total 100] train loss: 0.0128, val loss: 0.0224\n",
      "[epoch 25, total 100] train loss: 0.0100, val loss: 0.0190\n",
      "[epoch 27, total 100] train loss: 0.0105, val loss: 0.0153\n",
      "[epoch 29, total 100] train loss: 0.0076, val loss: 0.0131\n",
      "[epoch 31, total 100] train loss: 0.0067, val loss: 0.0137\n",
      "[epoch 33, total 100] train loss: 0.0093, val loss: 0.0156\n",
      "[epoch 35, total 100] train loss: 0.0061, val loss: 0.0094\n",
      "[epoch 37, total 100] train loss: 0.0056, val loss: 0.0118\n",
      "[epoch 39, total 100] train loss: 0.0044, val loss: 0.0080\n",
      "[epoch 41, total 100] train loss: 0.0044, val loss: 0.0073\n",
      "[epoch 43, total 100] train loss: 0.0038, val loss: 0.0080\n",
      "[epoch 45, total 100] train loss: 0.0038, val loss: 0.0079\n",
      "[epoch 47, total 100] train loss: 0.0036, val loss: 0.0063\n",
      "[epoch 49, total 100] train loss: 0.0029, val loss: 0.0062\n",
      "[epoch 51, total 100] train loss: 0.0029, val loss: 0.0064\n",
      "[epoch 53, total 100] train loss: 0.0029, val loss: 0.0059\n",
      "[epoch 55, total 100] train loss: 0.0030, val loss: 0.0069\n",
      "[epoch 57, total 100] train loss: 0.0024, val loss: 0.0055\n",
      "[epoch 59, total 100] train loss: 0.0024, val loss: 0.0091\n",
      "[epoch 61, total 100] train loss: 0.0039, val loss: 0.0052\n",
      "[epoch 63, total 100] train loss: 0.0042, val loss: 0.0060\n",
      "[epoch 65, total 100] train loss: 0.0031, val loss: 0.0084\n",
      "early stopping!\n",
      "Finished Training. Test loss:  0.004005380906164646\n"
     ]
    }
   ],
   "source": [
    "model, mlp_loss = train(\n",
    "    model, data_loaders, device=device, learning_rate=1e-2, l1_const=5e-5, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the MLP's learned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = get_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect interactions from the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise interactions              Arbitrary-order interactions\n",
      "(np.int64(16), np.int64(84))0.0000                      (np.int64(15), np.int64(18))0.0000        \n",
      "(np.int64(6), np.int64(16))0.0000                      (np.int64(15), np.int64(18), np.int64(92))0.0000        \n",
      "(np.int64(15), np.int64(29))0.0000                      (np.int64(6), np.int64(32))0.0000        \n",
      "(np.int64(6), np.int64(32))0.0000                      (np.int64(15), np.int64(18), np.int64(76), np.int64(92))0.0000        \n",
      "(np.int64(5), np.int64(18))0.0000                      (np.int64(6), np.int64(32), np.int64(37))0.0000        \n",
      "(np.int64(6), np.int64(29))0.0000                      (np.int64(15), np.int64(18), np.int64(35), np.int64(76), np.int64(92))0.0000        \n",
      "(np.int64(5), np.int64(29))0.0000                      (np.int64(3), np.int64(6), np.int64(32), np.int64(37))0.0000        \n",
      "(np.int64(29), np.int64(70))0.0000                      (np.int64(3), np.int64(6), np.int64(32), np.int64(37), np.int64(42))0.0000        \n",
      "(np.int64(29), np.int64(73))0.0000                      (np.int64(15), np.int64(18), np.int64(29), np.int64(35), np.int64(76), np.int64(92))0.0000        \n",
      "(np.int64(6), np.int64(84))0.0000                      (np.int64(15), np.int64(18), np.int64(29), np.int64(35), np.int64(76), np.int64(85), np.int64(92))0.0000        \n"
     ]
    }
   ],
   "source": [
    "anyorder_interactions = get_interactions(model_weights, one_indexed=True)\n",
    "pairwise_interactions = get_interactions(model_weights, pairwise=True, one_indexed=True)\n",
    "\n",
    "\n",
    "print_rankings(pairwise_interactions, anyorder_interactions, top_k=10, spacing=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise AUC 0.5105806451612903 , Any-order R-Precision 0.0\n"
     ]
    }
   ],
   "source": [
    "auc = get_pairwise_auc(pairwise_interactions, ground_truth)\n",
    "r_prec = get_anyorder_R_precision(anyorder_interactions, ground_truth)\n",
    "\n",
    "print(\"Pairwise AUC\", auc, \", Any-order R-Precision\", r_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}